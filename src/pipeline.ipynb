{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Human Motion Animation Generation Pipeline\n",
				"\n",
				"This notebook implements a pipeline for generating human motion animations using:\n",
				"\n",
				"- **Motion History Encoder**: Encodes motion context sequentially\n",
				"- **Flow Matching Network**: Generates motion sequences using flow matching\n",
				"\n",
				"- Input: HumanML3D dim-263 feature vectors + caption\n",
				"- Output: Joint positions (nframe, 22, 3) → BVH files\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Imports\n",
				"import torch\n",
				"import torch.nn as nn\n",
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"from pathlib import Path\n",
				"import sys\n",
				"from config import Config\n",
				"\n",
				"# Add project root to path\n",
				"sys.path.append(str(Path.cwd()))\n",
				"\n",
				"from config import Config\n",
				"from models import MotionHistoryEncoder, FlowMatchingNetwork\n",
				"from utils.utils import (\n",
				"    load_sample,\n",
				"    create_dataloader,\n",
				"    feature_to_joints,\n",
				"    joints_to_bvh,\n",
				"    save_bvh,\n",
				"    save_joints,\n",
				"    compute_metrics,\n",
				"    visualize_motion,\n",
				"    extract_features,\n",
				"    recover_from_ric,\n",
				")\n",
				"\n",
				"# Set device\n",
				"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
				"print(f\"Using device: {device}\")\n",
				"\n",
				"# Load configuration\n",
				"config = Config()\n",
				"config.dataset_path = Path(\"./dataset/humanml3d-subset\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"with open(config.dataset_path / \"all.txt\", \"r\") as f:\n",
				"    all_ids = f.readlines()\n",
				"\n",
				"file_id = all_ids[0].strip()  # Remove newline character\n",
				"data_path = config.dataset_path / \"new_joints\" / f\"{file_id}.npy\"\n",
				"motion_data = np.load(data_path)\n",
				"text_path = config.dataset_path / \"texts\" / f\"{file_id}.txt\"\n",
				"with open(text_path, \"r\") as f:\n",
				"    text = f.read()\n",
				"\n",
				"# 2. Visualize\n",
				"ani = visualize_motion(motion_data, title=f\"{file_id}.npy\", fps=20, skip_frames=2)\n",
				"ani"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 1: Data Preparation (HumanML3D)\n",
				"\n",
				"Load and preprocess the HumanML3D dataset with dim-263 feature vectors.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create dataloaders using the modular utilities\n",
				"print(\"Creating dataloaders...\")\n",
				"\n",
				"train_loader = create_dataloader(config, split=\"train\", shuffle=True)\n",
				"val_loader = create_dataloader(config, split=\"val\", shuffle=False)\n",
				"\n",
				"print(f\"Train dataloader created with batch_size={config.batch_size}\")\n",
				"print(f\"Val dataloader created with batch_size={config.batch_size}\")\n",
				"\n",
				"# Iterate through a batch to verify\n",
				"print(\"\\nSample batch from train_loader:\")\n",
				"for batch_idx, (\n",
				"    captions,\n",
				"    input_features,\n",
				"    motions,\n",
				"    joints,\n",
				"    lengths,\n",
				") in enumerate(train_loader):\n",
				"    print(f\"Batch {batch_idx}:\")\n",
				"    print(f\"  Captions (list): {len(captions)} samples\")\n",
				"    print(\n",
				"        f\"  Input features shape: {input_features.shape}\"\n",
				"    )  # (batch_size, max_length, feature_dim)\n",
				"    print(f\"  Motions shape: {motions.shape}\")  # (batch_size, max_length, 263)\n",
				"    print(f\"  Joints shape: {joints.shape}\")  # (batch_size, max_length, 22, 3)\n",
				"    print(f\"  Lengths shape: {lengths.shape}\")  # (batch_size,)\n",
				"\n",
				"    # Sample caption and motion\n",
				"    print(f\"  Sample caption: '{captions[0]}'\")\n",
				"    print(f\"  Sample input feature shape: {input_features[0].shape}\")\n",
				"    print(f\"  Sample motion shape: {motions[0].shape}\")\n",
				"    print(f\"  Sample joints shape: {joints[0].shape}\")\n",
				"    print(f\"  Sample length: {lengths[0].item()}\")\n",
				"\n",
				"    # Show one batch and break\n",
				"    if batch_idx == 0:\n",
				"        break"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 2: Autoregressive Context Encoder\n",
				"\n",
				"Initialize and test the autoregressive context encoder model.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Initialize Autoregressive Context Encoder\n",
				"context_encoder = MotionHistoryEncoder(\n",
				"    input_dim=config.motion_dim,  # 263\n",
				"    hidden_dim=config.hidden_dim,\n",
				"    num_layers=config.num_encoder_layers,\n",
				"    max_seq_length=config.max_motion_length,\n",
				"    bidirectional=config.bidirectional_gru,\n",
				").to(device)\n",
				"\n",
				"print(\n",
				"    f\"Context Encoder parameters: {sum(p.numel() for p in context_encoder.parameters()):,}\"\n",
				")\n",
				"\n",
				"# TODO: Test forward pass\n",
				"sample_batch_motion = torch.randn(\n",
				"    config.batch_size, config.max_motion_length, config.motion_dim\n",
				").to(device)\n",
				"sample_batch_text = [\"A person is walking\"] * config.batch_size\n",
				"\n",
				"with torch.no_grad():\n",
				"    context_output = context_encoder(sample_batch_motion, sample_batch_text)\n",
				"    print(f\"Context encoder output shape: {context_output.shape}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 3: Flow Matching Network\n",
				"\n",
				"Initialize and test the flow matching network model.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Initialize Flow Matching Network\n",
				"flow_matching_net = FlowMatchingNetwork(\n",
				"    context_dim=context_encoder.output_dim,\n",
				"    motion_dim=config.motion_dim,  # 263\n",
				"    hidden_dim=config.hidden_dim,\n",
				"    num_layers=config.num_flow_layers,\n",
				").to(device)\n",
				"\n",
				"print(\n",
				"    f\"Flow Matching Network parameters: {sum(p.numel() for p in flow_matching_net.parameters()):,}\"\n",
				")\n",
				"\n",
				"# TODO: Test forward pass\n",
				"with torch.no_grad():\n",
				"    # Flow matching forward pass\n",
				"    flow_output = flow_matching_net(context_output, sample_batch_motion)\n",
				"    print(f\"Flow matching output shape: {flow_output.shape}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 4: Training Loop\n",
				"\n",
				"Set up training configuration, loss functions, and training loop.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Set up optimizers\n",
				"optimizer_context = torch.optim.Adam(\n",
				"    context_encoder.parameters(), lr=config.learning_rate\n",
				")\n",
				"\n",
				"optimizer_flow = torch.optim.Adam(\n",
				"    flow_matching_net.parameters(), lr=config.learning_rate\n",
				")\n",
				"\n",
				"\n",
				"# TODO: Define loss functions\n",
				"def compute_loss(predicted_motion, target_motion, context_output):\n",
				"    \"\"\"\n",
				"    Compute training loss for flow matching.\n",
				"\n",
				"    Args:\n",
				"        predicted_motion: Generated motion from flow matching (batch, seq_len, 263)\n",
				"        target_motion: Ground truth motion (batch, seq_len, 263)\n",
				"        context_output: Context from autoregressive encoder\n",
				"\n",
				"    Returns:\n",
				"        loss: Scalar loss value\n",
				"    \"\"\"\n",
				"    # TODO: Implement flow matching loss\n",
				"    loss = nn.MSELoss()(predicted_motion, target_motion)\n",
				"    return loss\n",
				"\n",
				"\n",
				"# TODO: Training loop\n",
				"def train_epoch(\n",
				"    model_context, model_flow, train_loader, optimizer_context, optimizer_flow, device\n",
				"):\n",
				"    \"\"\"\n",
				"    Train for one epoch.\n",
				"    \"\"\"\n",
				"    model_context.train()\n",
				"    model_flow.train()\n",
				"\n",
				"    total_loss = 0.0\n",
				"    num_batches = 0\n",
				"\n",
				"    for batch_idx, (motion, text) in enumerate(train_loader):\n",
				"        # TODO: Move to device\n",
				"        motion = motion.to(device)\n",
				"\n",
				"        # TODO: Forward pass\n",
				"        # 1. Encode context\n",
				"        context = model_context(motion, text)\n",
				"\n",
				"        # 2. Flow matching\n",
				"        predicted_motion = model_flow(context, motion)\n",
				"\n",
				"        # 3. Compute loss\n",
				"        loss = compute_loss(predicted_motion, motion, context)\n",
				"\n",
				"        # TODO: Backward pass\n",
				"        optimizer_context.zero_grad()\n",
				"        optimizer_flow.zero_grad()\n",
				"        loss.backward()\n",
				"        optimizer_context.step()\n",
				"        optimizer_flow.step()\n",
				"\n",
				"        total_loss += loss.item()\n",
				"        num_batches += 1\n",
				"\n",
				"        if batch_idx % 100 == 0:\n",
				"            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
				"\n",
				"    return total_loss / num_batches\n",
				"\n",
				"\n",
				"# TODO: Validation loop\n",
				"def validate(model_context, model_flow, val_loader, device):\n",
				"    \"\"\"\n",
				"    Validate model performance.\n",
				"    \"\"\"\n",
				"    model_context.eval()\n",
				"    model_flow.eval()\n",
				"\n",
				"    total_loss = 0.0\n",
				"    num_batches = 0\n",
				"\n",
				"    with torch.no_grad():\n",
				"        for motion, text in val_loader:\n",
				"            motion = motion.to(device)\n",
				"\n",
				"            context = model_context(motion, text)\n",
				"            predicted_motion = model_flow(context, motion)\n",
				"            loss = compute_loss(predicted_motion, motion, context)\n",
				"\n",
				"            total_loss += loss.item()\n",
				"            num_batches += 1\n",
				"\n",
				"    return total_loss / num_batches\n",
				"\n",
				"\n",
				"# TODO: Training loop with checkpointing\n",
				"num_epochs = config.num_epochs\n",
				"best_val_loss = float(\"inf\")\n",
				"\n",
				"for epoch in range(num_epochs):\n",
				"    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
				"\n",
				"    # Train\n",
				"    train_loss = train_epoch(\n",
				"        context_encoder,\n",
				"        flow_matching_net,\n",
				"        train_loader,\n",
				"        optimizer_context,\n",
				"        optimizer_flow,\n",
				"        device,\n",
				"    )\n",
				"    print(f\"Train Loss: {train_loss:.4f}\")\n",
				"\n",
				"    # Validate\n",
				"    val_loss = validate(context_encoder, flow_matching_net, val_loader, device)\n",
				"    print(f\"Val Loss: {val_loss:.4f}\")\n",
				"\n",
				"    # TODO: Save checkpoint\n",
				"    if val_loss < best_val_loss:\n",
				"        best_val_loss = val_loss\n",
				"        torch.save(\n",
				"            {\n",
				"                \"context_encoder\": context_encoder.state_dict(),\n",
				"                \"flow_matching_net\": flow_matching_net.state_dict(),\n",
				"                \"epoch\": epoch,\n",
				"                \"val_loss\": val_loss,\n",
				"            },\n",
				"            config.checkpoint_dir / f\"best_model_epoch_{epoch+1}.pt\",\n",
				"        )\n",
				"        print(f\"Saved best model (val_loss: {val_loss:.4f})\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 5: Inference / Generation\n",
				"\n",
				"Load trained models and generate motion sequences.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Load trained models\n",
				"checkpoint_path = (\n",
				"    config.checkpoint_dir / \"best_model_epoch_X.pt\"\n",
				")  # Update with actual path\n",
				"checkpoint = torch.load(checkpoint_path, map_location=device)\n",
				"\n",
				"context_encoder.load_state_dict(checkpoint[\"context_encoder\"])\n",
				"flow_matching_net.load_state_dict(checkpoint[\"flow_matching_net\"])\n",
				"\n",
				"context_encoder.eval()\n",
				"flow_matching_net.eval()\n",
				"\n",
				"print(\"Models loaded successfully\")\n",
				"\n",
				"\n",
				"# TODO: Generate motion sequences\n",
				"def generate_motion(\n",
				"    model_context, model_flow, text_prompt, motion_length=None, device=\"cuda\"\n",
				"):\n",
				"    \"\"\"\n",
				"    Generate motion from text prompt.\n",
				"\n",
				"    Args:\n",
				"        model_context: Trained context encoder\n",
				"        model_flow: Trained flow matching network\n",
				"        text_prompt: Text description of desired motion\n",
				"        motion_length: Desired motion length in frames (optional)\n",
				"        device: Device to run on\n",
				"\n",
				"    Returns:\n",
				"        generated_motion: Generated motion as dim-263 features (seq_len, 263)\n",
				"    \"\"\"\n",
				"    model_context.eval()\n",
				"    model_flow.eval()\n",
				"\n",
				"    with torch.no_grad():\n",
				"        # TODO: Generate initial context from text\n",
				"        # For now, use random initialization - will be replaced with text encoding\n",
				"        if motion_length is None:\n",
				"            motion_length = config.max_motion_length\n",
				"\n",
				"        # TODO: Autoregressive generation with flow matching\n",
				"        # 1. Initialize with context\n",
				"        # 2. Iteratively generate using flow matching\n",
				"        # 3. Return generated motion sequence\n",
				"\n",
				"        # Placeholder: random generation for skeleton\n",
				"        generated_motion = torch.randn(motion_length, config.motion_dim).to(device)\n",
				"\n",
				"    return generated_motion.cpu().numpy()\n",
				"\n",
				"\n",
				"# TODO: Generate from text prompts\n",
				"text_prompts = [\n",
				"    \"A person is walking forward\",\n",
				"    \"A person is running on a treadmill\",\n",
				"    \"A person is dancing\",\n",
				"]\n",
				"\n",
				"generated_motions = []\n",
				"for text in text_prompts:\n",
				"    motion = generate_motion(context_encoder, flow_matching_net, text, device=device)\n",
				"    generated_motions.append(motion)\n",
				"    print(f\"Generated motion for: '{text}' - Shape: {motion.shape}\")\n",
				"\n",
				"# TODO: Convert to joint positions\n",
				"generated_joints = []\n",
				"for motion in generated_motions:\n",
				"    joints = feature_to_joints(motion)  # (nframe, 22, 3)\n",
				"    generated_joints.append(joints)\n",
				"    print(f\"Converted to joints - Shape: {joints.shape}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 6: Post-processing\n",
				"\n",
				"Convert generated motions to BVH format and save files.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Create output directories\n",
				"output_dir = Path(config.output_path) / \"experiment_1\"\n",
				"joints_dir = output_dir / \"joints\"\n",
				"animation_dir = output_dir / \"animation\"\n",
				"\n",
				"joints_dir.mkdir(parents=True, exist_ok=True)\n",
				"animation_dir.mkdir(parents=True, exist_ok=True)\n",
				"\n",
				"# TODO: Convert joint positions to BVH and save\n",
				"for idx, (joints, text) in enumerate(zip(generated_joints, text_prompts)):\n",
				"    # Save joint positions as numpy file\n",
				"    joints_file = joints_dir / f\"motion_{idx:04d}.npy\"\n",
				"    np.save(joints_file, joints)\n",
				"    print(f\"Saved joints to {joints_file}\")\n",
				"\n",
				"    # Convert to BVH format\n",
				"    bvh_data = joints_to_bvh(joints)\n",
				"\n",
				"    # Save BVH file\n",
				"    bvh_file = animation_dir / f\"motion_{idx:04d}.bvh\"\n",
				"    save_bvh(bvh_data, bvh_file)\n",
				"    print(f\"Saved BVH to {bvh_file}\")\n",
				"\n",
				"    # TODO: Validate BVH structure\n",
				"    is_valid = validate_bvh(bvh_file)\n",
				"    print(f\"BVH validation: {'Valid' if is_valid else 'Invalid'}\")\n",
				"\n",
				"print(f\"\\nAll outputs saved to {output_dir}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 7: Evaluation\n",
				"\n",
				"Compute evaluation metrics and visualize generated motions.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# TODO: Load ground truth motions for comparison\n",
				"# For evaluation, compare generated motions with ground truth from validation set\n",
				"val_motions = []\n",
				"val_texts = []\n",
				"\n",
				"for i in range(min(10, len(val_data))):  # Sample 10 validation motions\n",
				"    motion, text = val_data[i]\n",
				"    val_motions.append(motion)\n",
				"    val_texts.append(text)\n",
				"\n",
				"# Convert validation motions to joints\n",
				"val_joints = [feature_to_joints(motion) for motion in val_motions]\n",
				"\n",
				"\n",
				"# TODO: Compute evaluation metrics\n",
				"def evaluate_generated_motions(\n",
				"    generated_joints, ground_truth_joints, generated_texts, gt_texts\n",
				"):\n",
				"    \"\"\"\n",
				"    Compute evaluation metrics for generated motions.\n",
				"\n",
				"    Metrics:\n",
				"    - FID (Fréchet Inception Distance) - motion quality\n",
				"    - Diversity - motion variety\n",
				"    - R-Precision - text-motion alignment\n",
				"    \"\"\"\n",
				"    # TODO: Implement metrics computation\n",
				"    metrics = {\n",
				"        \"fid\": 0.0,  # Placeholder\n",
				"        \"diversity\": 0.0,  # Placeholder\n",
				"        \"r_precision\": 0.0,  # Placeholder\n",
				"    }\n",
				"    return metrics\n",
				"\n",
				"\n",
				"metrics = compute_metrics(generated_joints, val_joints, text_prompts, val_texts)\n",
				"print(\"\\nEvaluation Metrics:\")\n",
				"for metric_name, value in metrics.items():\n",
				"    print(f\"  {metric_name}: {value:.4f}\")\n",
				"\n",
				"# TODO: Visualize generated motions\n",
				"for idx, (joints, text) in enumerate(zip(generated_joints, text_prompts)):\n",
				"    print(f\"\\nVisualizing motion {idx+1}: '{text}'\")\n",
				"    visualize_motion(\n",
				"        joints, title=text, save_path=animation_dir / f\"vis_motion_{idx:04d}.png\"\n",
				"    )\n",
				"\n",
				"# TODO: Compare with ground truth\n",
				"print(\"\\nComparing generated vs ground truth:\")\n",
				"for idx in range(min(3, len(generated_joints))):\n",
				"    print(f\"\\nSample {idx+1}:\")\n",
				"    print(f\"  Generated: '{text_prompts[idx]}'\")\n",
				"    print(f\"  Ground Truth: '{val_texts[idx]}'\")\n",
				"\n",
				"    # Visualize comparison\n",
				"    visualize_motion(\n",
				"        generated_joints[idx],\n",
				"        ground_truth=val_joints[idx],\n",
				"        title=f\"Generated vs GT - {idx+1}\",\n",
				"        save_path=animation_dir / f\"comparison_{idx:04d}.png\",\n",
				"    )\n",
				"\n",
				"print(\"\\nEvaluation complete!\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.8.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
